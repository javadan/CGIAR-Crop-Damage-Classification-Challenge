{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/javadan/CGIAR-Crop-Damage-Classification-Challenge/blob/main/Image_tiling_CGIAR_2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2_7_XbdPyFoM",
        "outputId": "7d79e86f-7965-452d-a529-0d08f3103dcd"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting timm\n",
            "  Downloading timm-0.9.12-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.9.12\n",
            "cp: target '/content/cgiar' is not a directory\n",
            "Copying and unzipping files to local directory...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing images: 100%|██████████| 1304/1304 [12:18<00:00,  1.77it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weights tensor: tensor([  2.3493,   2.6517,   5.6584, 437.7974,  54.9067], device='cuda:0')\n",
            "Device: cuda\n",
            "Training efficientvit_l2.r384_in1k - Fold 0\n",
            "Starting training for efficientvit_l2.r384_in1k, fold 0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.359639</td>\n",
              "      <td>1.283190</td>\n",
              "      <td>2:59:54</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model exported successfully: /content/models//efficientvit_l2.r384_in1k_fold0_epoch0.pkl\n",
            "Submission file saved to: /content/models//submission_efficientvit_l2.r384_in1k_fold0_epoch0.csv\n",
            "Testing and submission saved for epoch 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/1 00:00&lt;?]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "      <progress value='37' class='' max='7946' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.47% [37/7946 00:44&lt;2:36:54 1.4255]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e6a0496acffb>\u001b[0m in \u001b[0;36m<cell line: 463>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;31m# Actual training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel_base\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_bases\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0mmodel_training\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_evaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_datetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_prep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Skipping training as .pth files exist for all model bases.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-e6a0496acffb>\u001b[0m in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(self, model_base, train_data, current_datetime, scores_file_path, test_and_submission, data_prep)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m                 \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    117\u001b[0m     scheds = {'lr': combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final),\n\u001b[1;32m    118\u001b[0m               'mom': combined_cos(pct_start, *(self.moms if moms is None else moms))}\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mParamScheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_opt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;31m# %% ../../nbs/14_callback.schedule.ipynb 50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_hypers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelFitException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_end_cleanup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_end_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelEpochException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_opt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelTrainException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36mall_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36mone_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_one_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelBatchException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_grad_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_grad_opt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_grad_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'backward'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelBackwardException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'step'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelStepException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_backward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "############################### MOUNT GDRIVE ###################################\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "############################### INSTALL TIMM ###################################\n",
        "\n",
        "!pip install timm\n",
        "\n",
        "import torch.multiprocessing as mp\n",
        "mp.set_start_method('spawn', force=True)\n",
        "\n",
        "############################### IMPORTS ########################################\n",
        "\n",
        "from tqdm import tqdm\n",
        "from fastai.vision.all import PILImage\n",
        "from fastcore.parallel import *\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from fastai.vision.all import *\n",
        "from timm import create_model\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from fastai.callback.all import CutMix\n",
        "from fastai.test_utils import *\n",
        "from datetime import datetime\n",
        "from fastai.losses import FocalLossFlat\n",
        "import torch.nn.functional as F\n",
        "import shutil\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import ToTensor\n",
        "import glob\n",
        "\n",
        "############################### HYPER PARAMS ###################################\n",
        "\n",
        "SEED = 42\n",
        "N_FOLDS = 3\n",
        "BATCH_SIZE = 20\n",
        "#IMGSZ = 384 #224 #384  #determined from model\n",
        "EPOCHS = 5\n",
        "INIT_LR = 3e-4 #2e-4\n",
        "NUM_WORKER = 8\n",
        "PATIENCE = 3\n",
        "\n",
        "\n",
        "model_bases = ['efficientvit_l2.r384_in1k']\n",
        "\n",
        "#                'tiny_vit_21m_224.dist_in22k',\n",
        "#                'caformer_s36.sail_in22k']\n",
        "\n",
        "MODEL_BASE = 'efficientvit_l2.r384_in1k'\n",
        "SAVE_NAME = MODEL_BASE\n",
        "model_scores = {}\n",
        "\n",
        "\n",
        "!cp /content/drive/MyDrive/cgiar/*.csv /content/cgiar\n",
        "\n",
        "############################### GLOBALS SETUP ##################################\n",
        "\n",
        "# Changed to save locally\n",
        "MODELS_DIR = '/content/models/'\n",
        "DATASET_DIR = '/content/cgiar/'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "         ################ SKIP TRAINING?  ########\n",
        "\n",
        "\n",
        "existing_model_files = {model_base: glob.glob(f'{MODELS_DIR}/{model_base}_*/{model_base}_fold*.pth') for model_base in model_bases}\n",
        "skip_training = all(len(files) > 0 for files in existing_model_files.values())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "current_datetime = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "MODELS_PATH = os.path.join(MODELS_DIR, f\"{MODEL_BASE}_{current_datetime}\")\n",
        "os.makedirs(MODELS_PATH, exist_ok=True)\n",
        "os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "set_seed(SEED, reproducible=True)\n",
        "\n",
        "############################### COPY TO LOCAL ##################################\n",
        "\n",
        "# Target directory where images will be copied\n",
        "local_image_dir = '/content/images'\n",
        "\n",
        "# Check if the directory exists and has files\n",
        "if not os.path.exists(local_image_dir) or not os.listdir(local_image_dir):\n",
        "    print(\"Copying and unzipping files to local directory...\")\n",
        "\n",
        "    # Create the target directory if it doesn't exist\n",
        "    if not os.path.exists(local_image_dir):\n",
        "        os.makedirs(local_image_dir)\n",
        "\n",
        "    # Copy the zip file\n",
        "    !cp /content/drive/MyDrive/cgiar/images/images.zip /content/\n",
        "\n",
        "    !cp /content/drive/MyDrive/cgiar/*.csv /content/cgiar\n",
        "\n",
        "    # Unzip the file into the target directory\n",
        "    !unzip -q /content/images.zip -d /content/images\n",
        "else:\n",
        "    print(\"Files already copied to local directory.\")\n",
        "\n",
        "\n",
        "\n",
        "############################### CUSTOM IMAGE DATASET ###########################\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, dataframe, label_mapping, vocab):\n",
        "        self.df = dataframe\n",
        "        self.to_tensor = ToTensor()  # Transformation to convert PILImage to tensor\n",
        "        self.label_mapping = label_mapping  # Mapping from string labels to integers\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = row['path']\n",
        "        label = self.label_mapping[row['target']]  # Convert label to integer\n",
        "        image = PILImage.create(img_path)\n",
        "        image_tensor = self.to_tensor(image)  # Convert to tensor\n",
        "        label_tensor = torch.tensor(label, dtype=torch.long)  # Convert label to tensor\n",
        "\n",
        "        return image_tensor, label_tensor\n",
        "\n",
        "    def new_empty(self):\n",
        "        return CustomImageDataset(pd.DataFrame(columns=self.df.columns), self.label_mapping, self.vocab)\n",
        "\n",
        "\n",
        "############################### CUSTOM LR SCHEDULE #############################\n",
        "class CustomLRSchedule(Callback):\n",
        "    def __init__(self, lr_reduction_factor=0.9):\n",
        "        self.lr_reduction_factor = lr_reduction_factor\n",
        "\n",
        "    def after_epoch(self):\n",
        "        # Reduce the learning rate after each epoch\n",
        "        for param_group in self.learn.opt.param_groups:\n",
        "            param_group['lr'] *= self.lr_reduction_factor\n",
        "\n",
        "\n",
        "############################### HELPER METHODS #################################\n",
        "\n",
        "\n",
        "\n",
        "############ Function to determine the input size for a given model\n",
        "def get_input_size_for_model(model_base):\n",
        "    if '384' in model_base:\n",
        "        return 384\n",
        "    elif '244' in model_base:\n",
        "        return 244\n",
        "    else:\n",
        "        return 224  # default size\n",
        "\n",
        "\n",
        "model_input_size = get_input_size_for_model(MODEL_BASE)\n",
        "\n",
        "\n",
        "############ Function to copy files to Google Drive\n",
        "def copy_to_gdrive(source, destination):\n",
        "    try:\n",
        "        !cp -r {source} {destination}\n",
        "        print(f\"Copied {source} to {destination} successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error copying {source} to {destination}: {e}\")\n",
        "\n",
        "\n",
        "############################### SET UP SCORES FILE #############################\n",
        "\n",
        "scores_file_path = '/content/drive/MyDrive/models/model_scores.csv'\n",
        "if not os.path.exists(scores_file_path):\n",
        "    with open(scores_file_path, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['DateTime', 'Model', 'Fold', 'Score'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############################### DATA PREPARATION ###############################\n",
        "class DataPreparation:\n",
        "    def __init__(self, dataset_dir, image_dir, chunk_size):\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.image_dir = image_dir\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_counts_per_class = defaultdict(int)  # Initialize chunk counts for each class\n",
        "\n",
        "    ############# CHUNK IT UP ##################################################\n",
        "\n",
        "\n",
        "    def process_image(self, file_path, output_dir, class_label):\n",
        "        # Ensure the output directory exists\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        image = PILImage.create(file_path)\n",
        "        w, h = image.size\n",
        "        image_id = os.path.basename(file_path).split('.')[0]\n",
        "\n",
        "        # Function to check if the chunk already exists\n",
        "        def chunk_exists(image_id, i, j, output_dir):\n",
        "            chunk_path = os.path.join(output_dir, f\"{image_id}_chunk_{i}_{j}.jpg\")\n",
        "            return os.path.exists(chunk_path)\n",
        "\n",
        "        # Convert output_dir to a relative path\n",
        "        relative_output_dir = os.path.relpath(output_dir)\n",
        "\n",
        "        # Check if chunks or resized image already exist\n",
        "        if self.chunk_size is None or (w <= self.chunk_size and h <= self.chunk_size):\n",
        "            resized_path = os.path.join(relative_output_dir, f\"{image_id}.jpg\")\n",
        "            # Resize and save the image if it doesn't exist\n",
        "            if not os.path.exists(resized_path):\n",
        "                resized_image = image.resize((self.chunk_size, self.chunk_size))\n",
        "                resized_image.save(resized_path)\n",
        "            yield resized_path, image_id\n",
        "            return\n",
        "\n",
        "        n_chunks_x = max(1, w // self.chunk_size)\n",
        "        n_chunks_y = max(1, h // self.chunk_size)\n",
        "\n",
        "        # Chunk count logic\n",
        "        if self.chunk_size is None or (w <= self.chunk_size and h <= self.chunk_size):\n",
        "            self.chunk_counts_per_class[class_label] += 1\n",
        "        else:\n",
        "            self.chunk_counts_per_class[class_label] += n_chunks_x * n_chunks_y\n",
        "\n",
        "        # Check if any chunk does not exist, then proceed with processing\n",
        "        if not all(chunk_exists(image_id, i, j, output_dir) for i in range(n_chunks_x) for j in range(n_chunks_y)):\n",
        "            for i in range(n_chunks_x):\n",
        "                for j in range(n_chunks_y):\n",
        "                    start_x = i * self.chunk_size\n",
        "                    start_y = j * self.chunk_size\n",
        "                    end_x = start_x + self.chunk_size\n",
        "                    end_y = start_y + self.chunk_size\n",
        "                    chunk = image.crop((start_x, start_y, end_x, end_y)).resize((self.chunk_size, self.chunk_size))\n",
        "                    chunk_path = os.path.join(relative_output_dir, f\"{image_id}_chunk_{i}_{j}.jpg\")\n",
        "                    chunk.save(chunk_path)\n",
        "                    yield chunk_path, image_id\n",
        "        else:\n",
        "            # If all chunks already exist, yield their paths\n",
        "            for i in range(n_chunks_x):\n",
        "                for j in range(n_chunks_y):\n",
        "                    chunk_path = os.path.join(relative_output_dir, f\"{image_id}_chunk_{i}_{j}.jpg\")\n",
        "                    yield chunk_path, image_id\n",
        "\n",
        "    def compute_class_weights(self):\n",
        "        total_chunks = sum(self.chunk_counts_per_class.values())\n",
        "        class_weights = {cls: total_chunks/count for cls, count in self.chunk_counts_per_class.items()}\n",
        "        return class_weights\n",
        "    ####### MAP THE CHUNKS TO THE DAMAGE AND ORIGINAL ##########################\n",
        "\n",
        "    def prepare_train_data(self, data, kfold, output_dir, batch_size=100, max_samples=None):\n",
        "        df = data.copy()\n",
        "        df['image_id'] = df['filename'].apply(lambda x: x.split('.')[0])\n",
        "        df = df.drop_duplicates(subset='image_id', keep='first')\n",
        "        df['target'] = df['damage']\n",
        "        df['fold'] = -1\n",
        "\n",
        "        if max_samples is not None:\n",
        "            df = df.sample(n=max_samples, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "        for i, (train_idx, val_idx) in enumerate(kfold.split(df, df['target'])):\n",
        "            df.loc[val_idx, 'fold'] = i\n",
        "\n",
        "        chunk_data = []\n",
        "\n",
        "\n",
        "        for batch_start in tqdm(range(0, len(df), batch_size), desc=\"Processing images\"):\n",
        "            batch_end = min(batch_start + batch_size, len(df))\n",
        "            for _, row in df.iloc[batch_start:batch_end].iterrows():\n",
        "                file_path = os.path.join(self.image_dir, row['filename'])\n",
        "                class_label = row['target']\n",
        "                for chunk_path, image_id in self.process_image(file_path, output_dir, class_label):\n",
        "                    if not os.path.exists(chunk_path):\n",
        "                        print(f\"File not found: {chunk_path}\")\n",
        "                        continue\n",
        "                    chunk_data.append({'image_id': image_id, 'path': chunk_path, 'target': row['target'], 'fold': row['fold']})\n",
        "\n",
        "        return pd.DataFrame(chunk_data)\n",
        "\n",
        "    ####### LOAD DATA INTO DATASETS ############################################\n",
        "\n",
        "    def get_dataloaders(self, train_data, fold):\n",
        "        train_df = train_data[train_data['fold'] != fold].reset_index(drop=True)\n",
        "        valid_df = train_data[train_data['fold'] == fold].reset_index(drop=True)\n",
        "\n",
        "        label_mapping = {label: idx for idx, label in enumerate(train_df['target'].unique())}\n",
        "        vocab = [label for label, idx in sorted(label_mapping.items(), key=lambda item: item[1])]\n",
        "\n",
        "        train_dataset = CustomImageDataset(train_df, label_mapping, vocab=vocab)\n",
        "        valid_dataset = CustomImageDataset(valid_df, label_mapping, vocab=vocab)\n",
        "\n",
        "        train_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=0, shuffle=True)\n",
        "        valid_dl = DataLoader(valid_dataset, batch_size=BATCH_SIZE, num_workers=0)\n",
        "\n",
        "        return DataLoaders(train_dl, valid_dl)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############################### TRAIN EVAL CODE\n",
        "\n",
        "class ModelTraining:\n",
        "    def __init__(self, models_dir, seed, n_folds, batch_size, imgsz, epochs, init_lr, num_worker, patience, device, num_classes):\n",
        "        self.models_dir = models_dir\n",
        "        self.seed = seed\n",
        "        self.n_folds = n_folds\n",
        "        self.batch_size = batch_size\n",
        "        self.imgsz = imgsz\n",
        "        self.epochs = epochs\n",
        "        self.init_lr = init_lr\n",
        "        self.num_worker = num_worker\n",
        "        self.patience = patience\n",
        "        self.device = device\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "\n",
        "    def train_and_evaluate_model(self, model_base, train_data, current_datetime, scores_file_path, test_and_submission, data_prep):\n",
        "        class_weights = data_prep.compute_class_weights()\n",
        "        weights = torch.FloatTensor([class_weights[t] for t in train_data['target'].unique()]).to(self.device)\n",
        "        print(\"Weights tensor:\", weights)\n",
        "        print(\"Device:\", self.device)\n",
        "\n",
        "        for fold in range(self.n_folds):\n",
        "            print(f\"Training {model_base} - Fold {fold}\")\n",
        "\n",
        "\n",
        "            model = create_model(model_base, pretrained=False, num_classes=self.num_classes).to(self.device)\n",
        "            dls = data_prep.get_dataloaders(train_data, fold)\n",
        "\n",
        "            learn = Learner(dls, model, loss_func=torch.nn.CrossEntropyLoss(weight=weights))\n",
        "            print(f\"Starting training for {model_base}, fold {fold}\")\n",
        "\n",
        "            for epoch in range(self.epochs):\n",
        "                learn.model.train()\n",
        "                learn.fit_one_cycle(1, self.init_lr)\n",
        "                learn.model.eval()\n",
        "                val_loss = learn.validate()[0]\n",
        "\n",
        "                export_path = f'{self.models_dir}/{model_base}_fold{fold}_epoch{epoch}.pkl'\n",
        "                try:\n",
        "                    learn.export(export_path)\n",
        "                    print(f\"Model exported successfully: {export_path}\")\n",
        "\n",
        "                    # Perform testing and save submission\n",
        "                    test_and_submission.test_and_save_submission(learn, f'{DATASET_DIR}Test.csv', f'{DATASET_DIR}SampleSubmission.csv', f'{self.models_dir}/submission_{model_base}_fold{fold}_epoch{epoch}.csv', model_base, self.device)\n",
        "                    print(f\"Testing and submission saved for epoch {epoch}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during model export or testing: {e}\")\n",
        "\n",
        "            print(f\"Training completed for {model_base}, fold {fold}.\")\n",
        "\n",
        "#################################### TESTING ###################################\n",
        "\n",
        "\n",
        "class TestAndSubmission:\n",
        "    def __init__(self, models_dir, dataset_dir, number_of_classes):\n",
        "        self.models_dir = models_dir\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.number_of_classes = number_of_classes\n",
        "\n",
        "    def predict_from_chunks(self, model, image_path, model_base, device):\n",
        "        image = PILImage.create(image_path)\n",
        "        chunk_size = get_input_size_for_model(model_base)\n",
        "        to_tensor = ToTensor()\n",
        "\n",
        "        w, h = image.size\n",
        "        n_chunks_x = w // chunk_size\n",
        "        n_chunks_y = h // chunk_size\n",
        "\n",
        "        model.eval()\n",
        "        all_probs = []\n",
        "        with torch.no_grad():\n",
        "            if w < chunk_size or h < chunk_size:\n",
        "                # Resize image to the required chunk_size\n",
        "                resized_image = image.resize((chunk_size, chunk_size))\n",
        "                resized_tensor = to_tensor(resized_image).unsqueeze(0).to(device)\n",
        "                logits = model(resized_tensor)\n",
        "                probs = F.softmax(logits, dim=1).cpu().numpy()\n",
        "                all_probs.append(probs)\n",
        "            else:\n",
        "                for i in range(n_chunks_x):\n",
        "                    for j in range(n_chunks_y):\n",
        "                        start_x, start_y = i * chunk_size, j * chunk_size\n",
        "                        end_x, end_y = start_x + chunk_size, start_y + chunk_size\n",
        "                        chunk = image.crop((start_x, start_y, end_x, end_y))\n",
        "                        chunk_tensor = to_tensor(chunk).unsqueeze(0).to(device)\n",
        "\n",
        "                        logits = model(chunk_tensor)\n",
        "                        probs = F.softmax(logits, dim=1).cpu().numpy()\n",
        "                        all_probs.append(probs)\n",
        "\n",
        "        # Take the mean across all chunk probabilities\n",
        "        mean_probs = np.mean(np.vstack(all_probs), axis=0)\n",
        "        return mean_probs\n",
        "\n",
        "\n",
        "    def test_and_save_submission(self, learn, test_df_path, submission_df_path, submission_output_path, model_base, device):\n",
        "        test_df = pd.read_csv(test_df_path)\n",
        "        test_df['path'] = test_df['filename'].map(lambda x: f'images/images/{x}')\n",
        "\n",
        "        all_probs = []\n",
        "        model = learn.model\n",
        "        for img_path in test_df['path']:\n",
        "            probs = self.predict_from_chunks(model, img_path, model_base, device)\n",
        "            all_probs.append(probs)\n",
        "\n",
        "        # Convert to a proper numpy array\n",
        "        all_probs = np.vstack(all_probs)\n",
        "\n",
        "        # Create the submission DataFrame\n",
        "        submission_df = pd.read_csv(submission_df_path)\n",
        "        for i, label in enumerate(learn.dls.vocab):\n",
        "            submission_df[label] = all_probs[:, i]  # Assign probabilities to each label\n",
        "\n",
        "        # Save the submission DataFrame to a CSV file\n",
        "        submission_df.to_csv(submission_output_path, index=False)\n",
        "        print(f\"Submission file saved to: {submission_output_path}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#################################### TOP LEVEL CODE ############################\n",
        "\n",
        "\n",
        "#################################### PREP TRAIN DATA ###########################\n",
        "\n",
        "# Get the device your model will be running on (either 'cuda' or 'cpu')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train = pd.read_csv(f'{DATASET_DIR}Train.csv')\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "output_dir = '/content/chunks'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "#No real elegant way to do this.\n",
        "# now that we're not always training.  unique_classes = train_data['target'].nunique()\n",
        "unique_classes = 5\n",
        "\n",
        "\n",
        "# Instantiate DataPreparation class\n",
        "data_prep = DataPreparation(DATASET_DIR, 'images/images/', model_input_size)\n",
        "# Instantiate ModelTraining class\n",
        "model_training = ModelTraining(MODELS_DIR, SEED, N_FOLDS, BATCH_SIZE, model_input_size, EPOCHS, INIT_LR, NUM_WORKER, PATIENCE, device, unique_classes)\n",
        "# Instantiate TestAndSubmission class\n",
        "test_sub = TestAndSubmission(MODELS_DIR, DATASET_DIR, unique_classes)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "MAX_SAMPLES = None # 250  # Limit to 10 images for quick testing\n",
        "\n",
        "if not skip_training:\n",
        "    train_data = data_prep.prepare_train_data(train, skf, output_dir, BATCH_SIZE, max_samples=MAX_SAMPLES)\n",
        "\n",
        "    # Actual training\n",
        "    for model_base in model_bases:\n",
        "        model_training.train_and_evaluate_model(model_base, train_data, current_datetime, scores_file_path, test_sub, data_prep)\n",
        "else:\n",
        "    print(\"Skipping training as .pth files exist for all model bases.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##############TESTING\n",
        "def find_recent_model_dirs(model_bases, models_dir):\n",
        "    recent_model_dirs = {}\n",
        "    for model_base in model_bases:\n",
        "        model_dirs = sorted(glob.glob(f'{models_dir}/{model_base}_*'), reverse=True)\n",
        "        for model_dir in model_dirs:\n",
        "            for fold in range(N_FOLDS):\n",
        "                pkl_files = glob.glob(f'{model_dir}/{model_base}_fold{fold}.pkl')\n",
        "                pth_files = glob.glob(f'{model_dir}/{model_base}_fold{fold}_epoch*.pth')\n",
        "                if pkl_files or pth_files:\n",
        "                    recent_model_dirs[model_base] = model_dir\n",
        "                    break\n",
        "            if model_base in recent_model_dirs:\n",
        "                break\n",
        "        if model_base not in recent_model_dirs:\n",
        "            recent_model_dirs[model_base] = None\n",
        "    return recent_model_dirs\n",
        "\n",
        "def get_model_architecture(model_name, num_classes):\n",
        "\n",
        "    model = create_model(model_name, pretrained=False, num_classes=num_classes)\n",
        "    return model\n",
        "\n",
        "def test_with_saved_models(model_bases, scores_file_path, device, test_sub, test_df, recent_model_dirs):\n",
        "    scores_df = pd.read_csv(scores_file_path)\n",
        "    model_weights = scores_df.groupby('Model')['Score'].min().apply(lambda x: 1/x).to_dict()\n",
        "    weighted_ensemble = None\n",
        "    models_found = False\n",
        "\n",
        "    for model_base in model_bases:\n",
        "        for fold in range(N_FOLDS):\n",
        "            pkl_path = f'{recent_model_dirs[model_base]}/{model_base}_fold{fold}.pkl'\n",
        "            pth_paths = glob.glob(f'{recent_model_dirs[model_base]}/{model_base}_fold{fold}_epoch*.pth')\n",
        "\n",
        "            if os.path.exists(pkl_path):\n",
        "                model_path = pkl_path\n",
        "            elif pth_paths:\n",
        "                model_path = pth_paths[0]  # Take the first file if there are multiple\n",
        "            else:\n",
        "                print(f\"No model file found for {model_base} fold {fold}\")\n",
        "                continue\n",
        "\n",
        "            models_found = True\n",
        "            try:\n",
        "                learn = load_learner(model_path, cpu=False)\n",
        "                learn.model.to(device)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading model from {model_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "            model_weight = model_weights[f'{model_base}_fold{fold}']\n",
        "            if weighted_ensemble is None:\n",
        "                weighted_ensemble = np.zeros((len(test_df), len(learn.dls.vocab)))\n",
        "\n",
        "            preds = np.array([test_sub.predict_from_chunks(learn, img_path, model_base, device) for img_path in test_df['path']])\n",
        "            weighted_ensemble += preds * model_weight\n",
        "\n",
        "    if not models_found:\n",
        "        print(\"No models were found for any base or fold. Cannot proceed with testing.\")\n",
        "        return None\n",
        "\n",
        "    normalized_preds = weighted_ensemble / sum(model_weights.values())\n",
        "\n",
        "    for i, label in enumerate(learn.dls.vocab):\n",
        "        test_df[label] = normalized_preds[:, i]\n",
        "\n",
        "    return test_df\n",
        "\n",
        "\n",
        "if skip_training:\n",
        "    recent_model_dirs = find_recent_model_dirs(model_bases, MODELS_DIR)\n",
        "    print(recent_model_dirs)\n",
        "\n",
        "    if all(dir is None for dir in recent_model_dirs.values()):\n",
        "        print(\"No models were found for any base or fold. Cannot proceed with testing.\")\n",
        "    else:\n",
        "        test_df = pd.read_csv(f'{DATASET_DIR}Test.csv')\n",
        "        test_df['path'] = test_df['filename'].map(lambda x: f'images/images/{x}')\n",
        "        test_df = test_with_saved_models(model_bases, scores_file_path, device, test_sub, test_df, recent_model_dirs)\n",
        "\n",
        "        if test_df is not None:\n",
        "            submission_dir = f\"{MODELS_PATH}/submission\"\n",
        "            os.makedirs(submission_dir, exist_ok=True)\n",
        "            submission_path = f\"{submission_dir}/{MODEL_BASE}_final.csv\"\n",
        "            sample_submission_df = pd.read_csv(f\"{DATASET_DIR}SampleSubmission.csv\")\n",
        "            sample_submission_df = sample_submission_df[['ID']]\n",
        "            sample_submission_df = pd.merge(sample_submission_df, test_df[['ID'] + list(test_df.columns.drop(['ID', 'path']))], on='ID')\n",
        "            sample_submission_df.to_csv(submission_path, index=False)\n",
        "            print(f\"Final submission file saved to: {submission_path}\")\n",
        "        else:\n",
        "            print(\"Testing could not be completed. No submission file created.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to try modify the weights after inference"
      ],
      "metadata": {
        "id": "gupmta7GGo07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the submission data\n",
        "file_path = '/content/models/submission_efficientvit_l2.r384_in1k_fold0_epoch0.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Original weights and modified weights\n",
        "original_weights = [2.3493, 2.6517, 5.6584, 437.7974, 54.9067]\n",
        "modified_weights = [1.5, 2.4, 25, 320, 70]\n",
        "\n",
        "# Adjust the probabilities\n",
        "for i in range(1, 6):  # Columns 1 to 5\n",
        "    adjustment_factor = modified_weights[i-1] / original_weights[i-1]\n",
        "    df.iloc[:, i] *= adjustment_factor\n",
        "\n",
        "# Normalize the probabilities so they sum to 1\n",
        "df.iloc[:, 1:6] = df.iloc[:, 1:6].div(df.iloc[:, 1:6].sum(axis=1), axis=0)\n",
        "\n",
        "df.iloc[:, 1:6] = df.iloc[:, 1:6].round(5)\n",
        "\n",
        "# Save the adjusted data\n",
        "adjusted_file_path = '/content/models/adjusted_submission.csv'\n",
        "df.to_csv(adjusted_file_path, index=False)\n"
      ],
      "metadata": {
        "id": "lw1PeZOrR_sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test submission with 1 0 0 0 0 instead of probabilities and see how many of each column is picked"
      ],
      "metadata": {
        "id": "iO8C50LNGvLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "file_path = '/content/models/adjusted_submission.csv'\n",
        "\n",
        "# Load the adjusted submission data\n",
        "df_submission = pd.read_csv(file_path)\n",
        "\n",
        "# Function to apply the transformation\n",
        "def apply_max_to_one_np(row):\n",
        "    numeric_row = row.iloc[1:].to_numpy()\n",
        "    max_val_index = np.argmax(numeric_row)\n",
        "    numeric_row[:] = 0\n",
        "    numeric_row[max_val_index] = 1\n",
        "    return pd.Series(np.concatenate(([row.iloc[0]], numeric_row)), index=row.index)\n",
        "\n",
        "# Ensure that all columns except 'ID' are of the type 'float64'\n",
        "for col in df_submission.columns:\n",
        "    if col != 'ID':\n",
        "        df_submission[col] = df_submission[col].astype('float64')\n",
        "\n",
        "# Apply the numpy-based function to the dataframe\n",
        "transformed_df_submission_np = df_submission.apply(apply_max_to_one_np, axis=1)\n",
        "\n",
        "# Count the number of 1s in each column\n",
        "column_counts = transformed_df_submission_np.iloc[:, 1:].sum()\n",
        "print(\"Column Counts:\")\n",
        "print(column_counts)\n",
        "\n",
        "# Save the transformed dataframe back to a CSV file\n",
        "output_path = '/content/models/transformed_submission.csv'\n",
        "transformed_df_submission_np.to_csv(output_path, index=False)\n",
        "\n",
        "# Output path of the saved file\n",
        "output_path\n"
      ],
      "metadata": {
        "id": "Y1-MDN9cD9GL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "8ffdb272-778d-436c-84e2-45a8dedc5312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column Counts:\n",
            "DR       1638\n",
            "G        3624\n",
            "ND       1110\n",
            "WD       1530\n",
            "other     761\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/models/transformed_submission.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMU+s/03UmgPEokBQmFZ3x4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}